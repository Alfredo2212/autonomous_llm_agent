Safety
Each improvement in model capabilities warrants commensurate improvements to safety. 
For OpenAI o3 and o4-mini, we completely rebuilt our safety training data, adding new 
refusal prompts in areas such as biological threats (biorisk), malware generation, and 
jailbreaks. This refreshed data has led o3 and o4-mini to achieve strong performance on 
our internal refusal benchmarks (e.g., instruction hierarchy⁠, jailbreaks). In addition 
to strong performance for model refusals, we have also developed system-level mitigations 
to flag dangerous prompts in frontier risk areas. Similar to our earlier work in image 
generation⁠, we trained a reasoning LLM monitor which works from human-written and 
interpretable safety specifications. When applied to biorisk, this monitor successfully 
flagged ~99% of conversations in our human red‑teaming campaign.

We stress tested both models with our most rigorous safety program to date. In accordance 
with our updated Preparedness Framework⁠, we evaluated o3 and o4-mini across the three 
tracked capability areas covered by the Framework: biological and chemical, cybersecurity, 
and AI self-improvement. Based on the results of these evaluations, we have determined 
that both o3 and o4‑mini remain below the Framework's "High" threshold in all three 
categories. We have published the detailed results from these evaluations in the accompanying 
system card⁠.⁠